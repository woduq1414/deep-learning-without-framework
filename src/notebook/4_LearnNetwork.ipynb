{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\b4\\7b\\e9\\a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\\import_ipynb-0.1.3-cp37-none-any.whl\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "!pip install import_ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from functions.ipynb\n",
      "importing Jupyter notebook from gradient.ipynb\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from functions import *\n",
    "from gradient import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from ..\\dataset\\mnist.ipynb\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import import_ipynb\n",
    "from dataset.mnist import load_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        \"\"\"\n",
    "            weight는 랜덤으로, bias는 0으로 초기화한다.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def net_numerical_gradient(self, x, t):\n",
    "  \n",
    "        \n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "train acc, test acc | 0.09608333333333334, 0.0927\n",
      "train acc, test acc | 0.79605, 0.7992\n",
      "train acc, test acc | 0.87915, 0.8851\n",
      "train acc, test acc | 0.89885, 0.9004\n",
      "train acc, test acc | 0.9079166666666667, 0.909\n",
      "train acc, test acc | 0.9129, 0.9156\n",
      "train acc, test acc | 0.9191666666666667, 0.9212\n",
      "train acc, test acc | 0.9235833333333333, 0.9257\n",
      "train acc, test acc | 0.9268166666666666, 0.9274\n",
      "train acc, test acc | 0.9285166666666667, 0.931\n",
      "train acc, test acc | 0.9326833333333333, 0.9336\n",
      "train acc, test acc | 0.9353333333333333, 0.9356\n",
      "train acc, test acc | 0.9378, 0.9369\n",
      "train acc, test acc | 0.9403166666666667, 0.9379\n",
      "train acc, test acc | 0.9426666666666667, 0.9399\n",
      "train acc, test acc | 0.9442333333333334, 0.942\n",
      "train acc, test acc | 0.9454, 0.9418\n",
      "train acc, test acc | 0.9480666666666666, 0.9449\n",
      "train acc, test acc | 0.94945, 0.9475\n",
      "train acc, test acc | 0.9510333333333333, 0.9467\n",
      "train acc, test acc | 0.9518833333333333, 0.9483\n",
      "train acc, test acc | 0.9534666666666667, 0.9484\n",
      "train acc, test acc | 0.9543166666666667, 0.9505\n",
      "train acc, test acc | 0.9558166666666666, 0.9504\n",
      "train acc, test acc | 0.9567333333333333, 0.9522\n",
      "train acc, test acc | 0.9577333333333333, 0.9521\n",
      "train acc, test acc | 0.9588833333333333, 0.9533\n",
      "train acc, test acc | 0.95965, 0.9537\n",
      "train acc, test acc | 0.9607, 0.9552\n",
      "train acc, test acc | 0.9611833333333333, 0.9563\n",
      "train acc, test acc | 0.96225, 0.9562\n",
      "train acc, test acc | 0.9632333333333334, 0.9571\n",
      "train acc, test acc | 0.9635, 0.9568\n",
      "train acc, test acc | 0.9647333333333333, 0.957\n",
      "train acc, test acc | 0.96465, 0.9578\n",
      "train acc, test acc | 0.9654, 0.9587\n",
      "train acc, test acc | 0.9659166666666666, 0.9598\n",
      "train acc, test acc | 0.9668666666666667, 0.959\n",
      "train acc, test acc | 0.9672, 0.9595\n",
      "train acc, test acc | 0.9679, 0.9601\n",
      "train acc, test acc | 0.9683666666666667, 0.9616\n",
      "train acc, test acc | 0.96895, 0.9624\n",
      "train acc, test acc | 0.9695, 0.962\n",
      "train acc, test acc | 0.9697833333333333, 0.9628\n",
      "train acc, test acc | 0.97075, 0.9634\n",
      "train acc, test acc | 0.97145, 0.9641\n",
      "train acc, test acc | 0.9713833333333334, 0.9645\n",
      "train acc, test acc | 0.9719333333333333, 0.9649\n",
      "train acc, test acc | 0.9725833333333334, 0.9649\n",
      "train acc, test acc | 0.9726, 0.965\n",
      "train acc, test acc | 0.9729333333333333, 0.9648\n",
      "train acc, test acc | 0.9737666666666667, 0.966\n",
      "train acc, test acc | 0.97405, 0.966\n",
      "train acc, test acc | 0.9746666666666667, 0.9657\n",
      "train acc, test acc | 0.9749666666666666, 0.9664\n",
      "train acc, test acc | 0.9752333333333333, 0.9665\n",
      "train acc, test acc | 0.97535, 0.9667\n",
      "train acc, test acc | 0.9759, 0.9679\n",
      "train acc, test acc | 0.97625, 0.9678\n",
      "train acc, test acc | 0.9766833333333333, 0.9681\n",
      "train acc, test acc | 0.9772166666666666, 0.9684\n",
      "train acc, test acc | 0.9775333333333334, 0.968\n",
      "train acc, test acc | 0.9775666666666667, 0.968\n",
      "train acc, test acc | 0.9778, 0.968\n",
      "train acc, test acc | 0.9778666666666667, 0.9682\n",
      "train acc, test acc | 0.9785833333333334, 0.9682\n",
      "train acc, test acc | 0.9788166666666667, 0.9683\n",
      "train acc, test acc | 0.9784666666666667, 0.9688\n",
      "train acc, test acc | 0.9789333333333333, 0.9688\n",
      "train acc, test acc | 0.97895, 0.9692\n",
      "train acc, test acc | 0.9793833333333334, 0.9691\n",
      "train acc, test acc | 0.9795333333333334, 0.9689\n",
      "train acc, test acc | 0.97965, 0.9683\n",
      "train acc, test acc | 0.9801, 0.9688\n",
      "train acc, test acc | 0.98035, 0.9691\n",
      "train acc, test acc | 0.9808, 0.9697\n",
      "train acc, test acc | 0.9810833333333333, 0.9689\n",
      "train acc, test acc | 0.9811666666666666, 0.9694\n",
      "train acc, test acc | 0.9814833333333334, 0.9699\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "\n",
    "x = np.arange(len(train_acc_list))\n",
    "\n",
    "# plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.bar(x, train_acc_list, label=\"train_acc\", color=\"orange\")\n",
    "plt.plot(x, test_acc_list, label='test_acc',color=\"red\",  marker='o')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_sample_data_set():\n",
    "    \n",
    "    \"\"\"\n",
    "        x는 0~999 정수이다.\n",
    "        정답은 x를 100으로 나눴을 때의 몫이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.random.randint(999, size=(100,1))\n",
    "\n",
    "    t_data = x.flatten() // 100\n",
    "    \n",
    "    # t_data 원핫 인코딩 코드\n",
    "    num = np.unique(t_data, axis=0)\n",
    "    num = num.shape[0]\n",
    "    t = np.eye(num)[t_data] \n",
    "    \n",
    "    return x,t\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = make_data_set(), make_data_set()\n",
    "\n",
    "\n",
    "INPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 30\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "train acc, test acc | 0.09333333333333334, 0.08766666666666667\n",
      "train acc, test acc | 0.31093333333333334, 0.30533333333333335\n",
      "train acc, test acc | 0.31106666666666666, 0.30593333333333333\n",
      "train acc, test acc | 0.26033333333333336, 0.25693333333333335\n",
      "train acc, test acc | 0.4884, 0.4891333333333333\n",
      "train acc, test acc | 0.271, 0.2673333333333333\n",
      "train acc, test acc | 0.3212, 0.3164\n",
      "train acc, test acc | 0.6954666666666667, 0.6938\n",
      "train acc, test acc | 0.6312, 0.6252\n",
      "train acc, test acc | 0.416, 0.4056666666666667\n",
      "train acc, test acc | 0.5805333333333333, 0.5785333333333333\n",
      "train acc, test acc | 0.5988666666666667, 0.6001333333333333\n",
      "===============\n",
      "train acc, test acc | 0.5988666666666667, 0.6001333333333333\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "\n",
    "x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
    "x_test = (x_test - np.mean(x_test)) / np.std(x_test)\n",
    "\n",
    "network = TwoLayerNet(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 60000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 300   # 미니배치 크기\n",
    "learning_rate = 0.04\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "print(iter_per_epoch)\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    \n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # 기울기 계산\n",
    "#     grad = network.net_numerical_gradient(x_batch, t_batch) # - 수치 미분법\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "\n",
    "#     if i % (iter_per_epoch) * 5 == 0:\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "        \n",
    "print(\"===============\")\n",
    "print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "\n",
    "x = np.arange(len(train_acc_list))\n",
    "\n",
    "# plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.bar(x, train_acc_list, label=\"train_acc\", color=\"orange\")\n",
    "plt.plot(x, test_acc_list, label='test_acc',color=\"red\",  marker='o')\n",
    "plt.xlabel(\"i\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}