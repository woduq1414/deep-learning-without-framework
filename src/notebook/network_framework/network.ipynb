{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from optimizer.ipynb\n",
      "importing Jupyter notebook from gradient.ipynb\n",
      "importing Jupyter notebook from functions.ipynb\n",
      "importing Jupyter notebook from layer.ipynb\n",
      "importing Jupyter notebook from initializer.ipynb\n",
      "importing Jupyter notebook from scaler.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import optimizer as Optimizer\n",
    "\n",
    "import gradient as Gradient\n",
    "from functions import *\n",
    "import layer as Layer\n",
    "import  initializer as Initializer \n",
    "import scaler as Scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../..\\dataset\\mnist.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import import_ipynb\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNet:\n",
    "    \n",
    "    \n",
    "    def __init__(self, weight_decay_lambda = 0, is_use_dropout = False, dropout_ratio = 0.5):\n",
    "        \n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.is_use_dropout = is_use_dropout\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        \n",
    "        self.params = {}\n",
    "        self.layers = OrderedDict()\n",
    "        self.lastLayer = Layer.IdentityWithLoss()\n",
    "        self.hiddenSizeList = []\n",
    "        self.prevDenseLayer = None\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def add_layer(self, layer, **kwargs):\n",
    "        if not isinstance(layer, Layer.LayerType):\n",
    "            raise BaseException(\"Layer required\")\n",
    "            \n",
    "        if type(self.lastLayer)==type(Layer.SoftmaxWithLoss()):\n",
    "            raise BaseException(\"Already last layer set\")\n",
    "        \n",
    "        \n",
    "        layer_len = len(self.layers)\n",
    "        \n",
    "        if isinstance(layer, Layer.Dense):\n",
    "            \n",
    "            input_size = layer.input_size\n",
    "            \n",
    "            if input_size == None and self.prevDenseLayer is not None:\n",
    "                input_size = self.prevDenseLayer.hidden_size\n",
    "                \n",
    "            hidden_size = layer.hidden_size\n",
    "            \n",
    "            \n",
    "            \n",
    "            weight_init_std = 0.01\n",
    "            initializer =layer.initializer\n",
    "            if isinstance( initializer , Initializer.Std):\n",
    "                weight_init_std = initializer.std\n",
    "            elif isinstance( initializer , Initializer.He):\n",
    "                weight_init_std = np.sqrt(2.0 / input_size)\n",
    "            elif isinstance( initializer , Initializer.Xavier):\n",
    "                weight_init_std = np.sqrt(1.0 / input_size)\n",
    "            \n",
    "            self.hiddenSizeList.append(hidden_size)\n",
    "            \n",
    "            self.params[f\"W{layer_len}\"]  =weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "            self.params[f\"b{layer_len}\"] = np.zeros(hidden_size)\n",
    "            \n",
    "            self.layers[f\"Affine{layer_len}\"] = Layer.Affine(self.params[f\"W{layer_len}\"], self.params[f\"b{layer_len}\"])\n",
    "            \n",
    "            self.prevDenseLayer = layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            if layer.activation is not None:\n",
    "                self.add_layer(layer.activation)\n",
    "                \n",
    "            if self.is_use_dropout:\n",
    "                self.layers['Dropout' + str(layer_len)] = Layer.Dropout(self.dropout_ratio)\n",
    "\n",
    "                \n",
    "        elif isinstance(layer ,Layer.Relu):\n",
    "            \n",
    "            self.layers[f\"Relu{layer_len}\"] = Layer.Relu()\n",
    "            \n",
    "        elif isinstance(layer ,Layer.Sigmoid):\n",
    "            \n",
    "            self.layers[f\"Sigmoid{layer_len}\"] = Layer.Sigmoid()\n",
    "            \n",
    "        elif isinstance(layer ,Layer.SoftmaxWithLoss):\n",
    "            \n",
    "#             self.layers[f\"SoftmaxWithLoss{layer_len}\"] = Layer.SoftmaxWithLoss()\n",
    "            self.lastLayer = layer\n",
    "    \n",
    "        elif isinstance(layer ,Layer.IdentityWithLoss):\n",
    "            \n",
    "#             self.layers[f\"SoftmaxWithLoss{layer_len}\"] = Layer.SoftmaxWithLoss()\n",
    "            self.lastLayer = layer\n",
    "    \n",
    "    \n",
    "        elif isinstance(layer ,Layer.BatchNormalization):\n",
    "            \n",
    "            self.layers[f\"BatchNormal{layer_len}\"] = Layer.BatchNormalization(\n",
    "                gamma =  np.ones(self.hiddenSizeList[-1]),\n",
    "                beta =  np.zeros(self.hiddenSizeList[-1])\n",
    "            )\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers.values():\n",
    "            \n",
    "            if isinstance(layer, Layer.BatchNormalization) or isinstance(layer, Layer.Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t, train_flg=False):\n",
    "        y = self.predict(x, train_flg)\n",
    "        \n",
    "        weight_decay = 0\n",
    "        for idx, (name, layer) in enumerate(self.layers.items()):\n",
    "            if isinstance(layer ,Layer.Affine):\n",
    "                layer_num = name[6:]\n",
    "                W = self.params['W' + str(layer_num)]\n",
    "                weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "          \n",
    "        \n",
    "        return self.lastLayer.forward(y, t) + weight_decay\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "\n",
    "        \n",
    "        y = self.predict(x, train_flg=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, train_flg = True)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for idx, (name, layer) in enumerate(self.layers.items()):\n",
    "            if isinstance(layer ,Layer.Affine):\n",
    "                layer_num = name[6:]\n",
    "                grads[f\"W{layer_num}\"] = layer.dW + self.weight_decay_lambda * self.params['W' + str(layer_num)]\n",
    "                grads[f\"b{layer_num}\"] = layer.db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, t_train, x_test, t_test, **kwargs): # t_data 는 원 핫 인코딩\n",
    "        \n",
    "        optimizer = Optimizer.SGD(lr=0.01)\n",
    "        if \"optimizer\" in kwargs:\n",
    "            if isinstance(kwargs[\"optimizer\"], Optimizer.OptimizerType) :\n",
    "                optimizer = kwargs[\"optimizer\"]\n",
    "            else:\n",
    "                raise \"err\"\n",
    "                \n",
    "        iters_num = 10000\n",
    "        if \"iters_num\" in kwargs:\n",
    "            if 0 < kwargs[\"iters_num\"] :\n",
    "                iters_num = kwargs[\"iters_num\"]\n",
    "            else:\n",
    "                raise \"err\"\n",
    "                \n",
    "        batch_size = 100\n",
    "        if \"batch_size\" in kwargs:\n",
    "            if 0 < kwargs[\"batch_size\"] :\n",
    "                batch_size = kwargs[\"batch_size\"]\n",
    "            else:\n",
    "                raise \"err\"\n",
    "                \n",
    "        print_epoch = 1\n",
    "        if \"print_epoch\" in kwargs:\n",
    "            if 0 < kwargs[\"print_epoch\"] :\n",
    "                print_epoch = kwargs[\"print_epoch\"]\n",
    "            else:\n",
    "                raise \"err\"\n",
    "        \n",
    "        output_type = \"regression\"\n",
    "        if type(self.lastLayer)==type(Layer.SoftmaxWithLoss()):\n",
    "            output_type = \"class\"\n",
    "\n",
    "        train_size = x_train.shape[0]\n",
    "\n",
    "\n",
    "        train_loss_list = []\n",
    "        train_acc_list = []\n",
    "        test_acc_list = []\n",
    "\n",
    "        iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "        \n",
    "        print(f\"repeat { int(iters_num // (max(train_size / batch_size, 1))) } epoch\")\n",
    "        \n",
    "        if output_type == \"regression\":\n",
    "            print(\"epoch | loss\")\n",
    "        else:\n",
    "            print(\"epoch | train_acc | test_acc\")\n",
    "        \n",
    "        \n",
    "        cnt = 1\n",
    "        for i in range(iters_num):\n",
    "            batch_mask = np.random.choice(train_size, batch_size)\n",
    "            x_batch = x_train[batch_mask]\n",
    "            t_batch = t_train[batch_mask]\n",
    "\n",
    "\n",
    "            grad = self.gradient(x_batch, t_batch)\n",
    "\n",
    "            # 갱신\n",
    "            \n",
    "            optimizer.update(self.params, grad)\n",
    "\n",
    "            \n",
    "    \n",
    "            loss = self.loss(x_batch, t_batch)\n",
    "            train_loss_list.append(loss)\n",
    "      \n",
    "            \n",
    "            if int(cnt * iter_per_epoch) == i:\n",
    "                \n",
    "                if cnt % print_epoch == 0:\n",
    "                   \n",
    "                    train_acc = self.accuracy(x_train, t_train)\n",
    "                    test_acc = self.accuracy(x_test, t_test)\n",
    "                    train_acc_list.append(train_acc)\n",
    "                    test_acc_list.append(test_acc)\n",
    "                    \n",
    "                    if output_type == \"regression\":\n",
    "                        print(f\"epoch {cnt}:\",   train_loss_list[-1])\n",
    "                    else:\n",
    "                        print(f\"epoch {cnt}:\",  format(train_acc, \".4f\"), \" | \",  format(test_acc, \".4f\"))\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                cnt += 1\n",
    "                \n",
    "        print(\"===================\")\n",
    "        train_acc = self.accuracy(x_train, t_train)\n",
    "        test_acc = self.accuracy(x_test, t_test)\n",
    "        if output_type == \"regression\":\n",
    "            print(f\"epoch {cnt - 1}:\",   train_loss_list[-1])\n",
    "        else:\n",
    "            print(f\"epoch {cnt - 1}:\",  format(train_acc, \".4f\"), \" | \",  format(test_acc, \".4f\"))\n",
    "    \n",
    "        \n",
    "        return {\n",
    "            \"train_loss_list\" : train_loss_list,\n",
    "            \"train_acc_list\" : train_acc_list,\n",
    "            \"test_acc_list\" : test_acc_list\n",
    "        }\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
