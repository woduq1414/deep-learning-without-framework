{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from optimizer.ipynb\n",
      "importing Jupyter notebook from gradient.ipynb\n",
      "importing Jupyter notebook from functions.ipynb\n",
      "importing Jupyter notebook from layer.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from scaler.ipynb\n",
      "importing Jupyter notebook from ../..\\dataset\\mnist.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import optimizer as Optimizer\n",
    "\n",
    "import gradient as Gradient\n",
    "from functions import *\n",
    "import layer as Layer\n",
    "from network import MultiLayerNet\n",
    "\n",
    "import scaler as Scaler\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import import_ipynb\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def make_sample_data_set():\n",
    "    \n",
    "    \"\"\"\n",
    "        x는 0~999999 정수이다.\n",
    "        정답은 x를 100000으로 나눴을 때의 몫이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.random.randint(999999, size=(10000,1))\n",
    "    \n",
    "    t_data = x.flatten() // 100000\n",
    "    \n",
    "    \n",
    "    # t_data 원핫 인코딩 코드\n",
    "    num = np.unique(t_data, axis=0)\n",
    "    num = num.shape[0]\n",
    "    t = np.eye(num)[t_data] \n",
    "    \n",
    "    return x,t\n",
    "\n",
    "def make_sample_data_set_regression():\n",
    "    \n",
    "\n",
    "    x = np.random.randint(9, size=(300, 1))\n",
    "    y =np.dot(x, np.array([3]) )\n",
    "    t = np.reshape(y, (y.shape[0],1))\n",
    "\n",
    "    \n",
    "    return x,t\n",
    "\n",
    "def make_sample_data_set_regression2():\n",
    "    \n",
    "\n",
    "    x = np.random.randint(999, size=(300, 2))\n",
    "    y =np.dot(x, np.array([3,-2]) ) + 7\n",
    "#     y =  y + (6 * np.random.random_sample((1,y.shape[0])) - 3).flatten()\n",
    "    t = np.reshape(y, (y.shape[0],1))\n",
    "\n",
    "    \n",
    "    return x,t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 2) (240, 1) (60, 2) (60, 1)\n",
      "repeat 4 epoch\n",
      "epoch | loss\n",
      "epoch 1: 80.42587292769434\n",
      "epoch 2: 0.3770490457642343\n",
      "epoch 3: 3.3780241896205944e-07\n",
      "epoch 4: 3.702744902720857e-11\n",
      "===================\n",
      "epoch 4: 3.702744902720857e-11\n",
      "target : [[1200  500]] result :  [[2606.99998597]]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "x_data, t_data = make_sample_data_set_regression2()\n",
    "\n",
    "net = MultiLayerNet()\n",
    "net.add_layer(Layer.Dense(1, input_size = 2, activation=Layer.IdentityWithLoss() ))\n",
    "# net.add_layer(Layer.Dense(5, input_size = 2, activation=Layer.Relu() ))\n",
    "# net.add_layer(Layer.Dense(1))\n",
    "\n",
    "\n",
    "x_train, t_train, x_test, t_test = shuffle_split_data(x_data, t_data, 0.2)\n",
    "\n",
    "print(x_train.shape, t_train.shape, x_test.shape, t_test.shape)\n",
    "\n",
    "scaler = Scaler.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "result = net.train(\n",
    "        x_train, t_train, x_test, t_test, batch_size = 100, iters_num = 10, print_epoch = 1,\n",
    "        optimizer = Optimizer.SGD()\n",
    ")\n",
    "\n",
    "predict_target = np.array([[1200, 500]])\n",
    "print(\"target :\" ,predict_target, \"result : \",  net.predict(scaler.transform(predict_target)))\n",
    "\n",
    "\n",
    "print(\"done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n",
      "repeat 15 epoch\n",
      "epoch | train_acc | test_acc\n",
      "epoch 1: 0.9590  |  0.9538\n",
      "epoch 2: 0.9722  |  0.9664\n",
      "epoch 3: 0.9815  |  0.9716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8c96565dcb0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m result = net.train(\n\u001b[0;32m     15\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-without-tensorflow\\src\\notebook\\network_framework\\network.ipynb\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, t_train, x_test, t_test, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape, t_train.shape, x_test.shape, t_test.shape)\n",
    "\n",
    "x_data = np.append(x_train, x_test, axis=0)\n",
    "t_data = np.append(t_train, t_test, axis=0)\n",
    "\n",
    "net = MultiLayerNet()\n",
    "net.add_layer(Layer.Dense(128, input_size = 784, activation = Layer.Relu() ))\n",
    "net.add_layer(Layer.Dense(10,  activation = Layer.SoftmaxWithLoss() ))\n",
    "\n",
    "result = net.train(\n",
    "        x_train, t_train, x_test, t_test, batch_size = 300, iters_num = 3000, print_epoch = 1,\n",
    "        optimizer = Optimizer.Adam(lr=0.01)\n",
    ")\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
